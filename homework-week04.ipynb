{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce608ad",
   "metadata": {},
   "source": [
    "# LSDL CUB, Homework 3. Constrastive Learning [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122249",
   "metadata": {},
   "source": [
    "This task is dedicated to contrastive self-supervised methods. We will focus on the SimCLR and BYOL algorithms that were discussed in class. We will conduct experiments on the [STL10](https://cs.stanford.edu/~acoates/stl10/) dataset, which is ideal for pretraining without labels, as it contains 100k unlabeled, 5k training labeled and 8k test labeled images.\n",
    "\n",
    "To submit the task, you must conduct the experiments described in this notebook and write a report on them in PDF format. Along with the report, the code that allows you to run the experiments must be submitted. Before implementing anything, read all the experiment statements and think about how to better organize the code, do not forget to checkpoint the necessary trained models. We reserve the right to lower the grade for poorly structured code. Be sure to use the **training optimizations** from the [list](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html) in your pipelines, for example, Automatic Mixed Precision, to speed up the experiments. Also note that the report is a **mandatory part** of the assessment, without it we will not check your assignment. The report must include training curves for all the models you run. Also, make sure that your figures and graphs are readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0716cb",
   "metadata": {},
   "source": [
    "## 0. Supervised baseline [0 pts]\n",
    "\n",
    "**If not completed, the maximum for the entire task is 0 points**\n",
    "\n",
    "We will start our study by training a supervised model from a random initial approximation. Use the labeled train for training, and the labeled test for testing. We will use ResNet-18\\* as the neural network architecture. We recommend searching for hyperparameters and augmentations for training on STL-10 in articles. The author of the task got an accuracy of about 71-72%.\n",
    "\n",
    "\\**For datasets with a smaller image size than ImageNet (such as CIFAR-10/100, STL-10), it is common to use ResNet-18 with modified first layers: usually, 7x7 convolution is replaced with 3x3, and 2x2 MaxPooling is removed. We suggest you train the regular ResNet-18 in the torchvision implementation to save time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ed8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import trange\n",
    "import wandb\n",
    "\n",
    "PROJECT_NAME = \"HW 3, LSDL 2024. CUB.\"\n",
    "\n",
    "def finetune_model(model, train_loader, test_loader, args, device, save_name):\n",
    "    wandb.init(project=PROJECT_NAME, name=save_name)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args['LEARNING_RATE'], momentum=args['MOMENTUM'], weight_decay=args['WEIGHT_DECAY'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['LR_GAMMA'])\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    log_it = 0\n",
    "\n",
    "    for i in trange(args['EPOCHS']):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            wandb.log({\"ft_loss\": loss.item()}, step=log_it)\n",
    "            log_it += 1\n",
    "\n",
    "        model.eval()\n",
    "        if (i + 1) % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                correct, total = 0, 0\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                # print(f'Epoch {i+1}/{EPOCHS}, Test Accuracy: {accuracy:.2f}')\n",
    "            wandb.log({\"test_accuracy\": accuracy}, step=log_it)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{save_name}.pth\")\n",
    "    wandb.finish()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063fe534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtgritsaev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hdilab/tgritsaev/other/pretext/wandb/run-20241104_160546-9f50gic4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/9f50gic4' target=\"_blank\">supervised</a></strong> to <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/9f50gic4' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/9f50gic4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:13<00:00,  6.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ft_loss</td><td>█▅▄▅▄▄▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▄▂▂▁▁▁▂▂▁▁▁▂▁▁▂▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▂▃▄▅▅▆▆▇▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ft_loss</td><td>0.05873</td></tr><tr><td>test_accuracy</td><td>71.425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">supervised</strong> at: <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/9f50gic4' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/9f50gic4</a><br/> View project at: <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_160546-9f50gic4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "71.425"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "args = {}\n",
    "args['LEARNING_RATE'] = 0.1\n",
    "args['MOMENTUM'] = 0.9\n",
    "args['WEIGHT_DECAY'] = 5e-4\n",
    "args['LR_GAMMA'] = 0.955\n",
    "args['EPOCHS'] = 100\n",
    "\n",
    "# Configuration\n",
    "NUM_WORKERS = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# Main code\n",
    "model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "\n",
    "mean_stats = [0.485, 0.456, 0.406]\n",
    "std_stats = [0.229, 0.224, 0.225]\n",
    "# from the previous homework\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats),  # Normalize the tensor,\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats)  # Normalize the tensor\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.STL10(root=\"./\", split='train', download=True, transform=train_transforms)\n",
    "test_set = torchvision.datasets.STL10(root=\"./\", split='test', download=True, transform=test_transforms)    \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "finetune_model(model, train_loader, test_loader, args, device, \"supervised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ad13e",
   "metadata": {},
   "source": [
    "## 1. SimCLR [1.5 pts]\n",
    "\n",
    "Implement and train the SimCLR method from [Chen et al, 2020](https://arxiv.org/pdf/2002.05709.pdf). We want you to implement it yourself, so this task does not allow you to borrow code from open sources. Use the unlabeled part of STL-10 as a training set, and use the labeled train to validate the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9271e6",
   "metadata": {},
   "source": [
    "## 2. BYOL [2.5 pts]\n",
    "\n",
    "Similar to the previous task, implement and train the BYOL method from [Grill et al, 2020](https://arxiv.org/pdf/2006.07733.pdf). To check that the projections do not collapse (the variable $z$ from the original paper), plot the standard deviation of $z$ throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING_RATE: 0.15\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xyn1a70m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>simclr_loss</td><td>▅█▇▅▅▃▇▆▅▅▄▆▇▅▄█▅▇▅▅▅▇▆▅▅▇▆▃▆▁▆▃▄▅▆▄▅▅▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>simclr_loss</td><td>4.08713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">simclr</strong> at: <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/xyn1a70m' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB./runs/xyn1a70m</a><br/> View project at: <a href='https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.' target=\"_blank\">https://wandb.ai/tgritsaev/HW%203%2C%20LSDL%202024.%20CUB.</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_202415-xyn1a70m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xyn1a70m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# From the paper:\n",
    "# Default setting. Unless otherwise specified, for data augmentation we use random crop and resize (with random\n",
    "# flip), color distortions, and Gaussian blur (for details, see\n",
    "# Appendix A). We use ResNet-50 as the base encoder network, and a 2-layer MLP projection head to project the\n",
    "# representation to a 128-dimensional latent space. As the\n",
    "# loss, we use NT-Xent, optimized using LARS with learning\n",
    "# rate of 4.8 (= 0.3 × BatchSize/256) and weight decay of\n",
    "# 10−6\n",
    "# . We train at batch size 4096 for 100 epochs.3 Furthermore, we use linear warmup for the first 10 epochs,\n",
    "# and decay the learning rate with the cosine decay schedule\n",
    "# without restarts (Loshchilov & Hutter, 2016).\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "# BATCH_SIZE = 4\n",
    "# LEARNING_RATE = 3e-4\n",
    "LEARNING_RATE = 0.3 * BATCH_SIZE / 256\n",
    "print(f\"LEARNING_RATE: {LEARNING_RATE}\")\n",
    "SIMCLR_EPOCHS = 100\n",
    "NUM_WARMUP_STEPS = 10\n",
    "\n",
    "TEMPERATURE = 0.1  # optimal temperature from the paper\n",
    "TEMPERATURE = 1\n",
    "\n",
    "# Appendix A:\n",
    "# In our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random\n",
    "# flip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations\n",
    "# are provided below.\n",
    "\n",
    "# Random crop and resize to 224x224 We use standard Inception-style random cropping (Szegedy et al., 2015). The\n",
    "# crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of\n",
    "# 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to the original size. This has been implemented in Tensorflow as “slim.preprocessing.inception_preprocessing.distorted_bounding_box_crop”, or in Pytorch\n",
    "# as “torchvision.transforms.RandomResizedCrop”. Additionally, the random crop (with resize) is always followed by a\n",
    "# random horizontal/left-to-right flip with 50% probability. This is helpful but not essential. By removing this from our default\n",
    "# augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs\n",
    "\n",
    "# Color distortion Color distortion is composed by color jittering and color dropping. We find stronger color jittering\n",
    "# usually helps, so we set a strength parameter.\n",
    "\n",
    "# Gaussian blur This augmentation is in our default policy. We find it helpful, as it improves our ResNet-50 trained for\n",
    "# 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample\n",
    "# σ ∈ [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\n",
    "\n",
    "\n",
    "# from the orginal paper\n",
    "def get_color_distortion(s=1.0):\n",
    "    # s is the strength of color distortion.\n",
    "    color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort\n",
    "\n",
    "\n",
    "simclr_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        # transforms.RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(3 / 4, 4 / 3)),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # get_color_distortion(0.3),\n",
    "        # transforms.GaussianBlur(kernel_size=int(23)),\n",
    "        transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "        transforms.Normalize(mean=mean_stats, std=std_stats),  # Normalize the tensor,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class DoubleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.dataset[idx]\n",
    "        image1 = self.transform(image)\n",
    "        image2 = self.transform(image)\n",
    "        return torch.stack([image1, image2])\n",
    "\n",
    "\n",
    "simclr_set = DoubleImageDataset(\n",
    "    torchvision.datasets.STL10(root=\"./\", split=\"unlabeled\", download=True), transform=simclr_transforms\n",
    ")\n",
    "simclr_loader = torch.utils.data.DataLoader(simclr_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Model for SimCLR\n",
    "output_after_avgpool = None\n",
    "\n",
    "\n",
    "class SimCLRResNet18(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimCLRResNet18, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet18(pretrained=False)\n",
    "        num_ftrs = self.encoder.fc.in_features\n",
    "        self.encoder.fc = torch.nn.Identity()  # Remove the original FC layer\n",
    "        self.proj_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_ftrs, 1024), torch.nn.ReLU(), torch.nn.Linear(1024, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.proj_head(h)\n",
    "        return z\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = SimCLRResNet18().to(device)\n",
    "\n",
    "val_set = torchvision.datasets.STL10(root=\"./\", split=\"train\", download=True, transform=test_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "# Create the LambdaLR scheduler with the warmup lambda function\n",
    "def linear_warmup_lambda(current_step):\n",
    "    if current_step < NUM_WARMUP_STEPS:\n",
    "        # Linear increase: current_step / num_warmup_steps scales from 0 to 1\n",
    "        return current_step / NUM_WARMUP_STEPS\n",
    "    else:\n",
    "        # After warmup, maintain the base learning rate\n",
    "        return args[\"LR_GAMMA\"] ** (current_step - NUM_WARMUP_STEPS)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(list(model.parameters()), lr=LEARNING_RATE, weight_decay=1e-6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=linear_warmup_lambda)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "wandb.init(project=PROJECT_NAME, name=\"simclr\")\n",
    "\n",
    "log_it = 0\n",
    "for i in trange(SIMCLR_EPOCHS):\n",
    "    model.train()\n",
    "    for images in simclr_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        images = images.reshape(-1, *images.shape[2:]).to(device)\n",
    "        bs = images.shape[0]\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            z = model(images)\n",
    "            # print(z.shape)\n",
    "            norm_z = z / z.norm(dim=1).unsqueeze(1)\n",
    "            pairwise_sim = torch.matmul(norm_z, norm_z.T) / TEMPERATURE\n",
    "            # print(pairwise_sim)\n",
    "            # print(pairwise_sim.shape)\n",
    "\n",
    "            # for k in range(bs // 2):\n",
    "            #     idx = 2 * k\n",
    "            #     losses[idx] = -(pairwise_sim[idx, idx + 1] - torch.log(sum_exp_pairwise_sim[idx] - pairwise_sim[idx, idx].exp()))\n",
    "\n",
    "            #     idx = 2 * k + 1\n",
    "            #     losses[idx] = -(pairwise_sim[idx, idx - 1] - torch.log(sum_exp_pairwise_sim[idx] - pairwise_sim[idx, idx].exp()))\n",
    "            # loss = -torch.log(positive_pairs / neg_exp_sum).mean()\n",
    "\n",
    "            # Vectorized loss calculation for NT-Xent loss\n",
    "            positive_pairs_sim = torch.stack([torch.diag(pairwise_sim, 1)[::2], torch.diag(pairwise_sim, -1)[::2]], dim=1).flatten()\n",
    "            # print(f\"{-positive_pairs_sim=}\")\n",
    "            sum_exp_pairwise_sim = torch.exp(pairwise_sim).sum(-1) - torch.diag(pairwise_sim).exp()\n",
    "            # print(f\"{sum_exp_pairwise_sim=}\")\n",
    "            # print(f\"{(torch.log(sum_exp_pairwise_sim - torch.diag(pairwise_sim).exp()))=}\")\n",
    "            # print(positive_pairs_sim.shape, sum_exp_pairwise_sim.shape, torch.diag(pairwise_sim).exp().shape)\n",
    "            loss = (-positive_pairs_sim + torch.log(sum_exp_pairwise_sim)).mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        wandb.log({\"simclr_loss\": loss.item()}, step=log_it)\n",
    "        log_it += 1\n",
    "\n",
    "    ft_model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "    ft_model.load_state_dict({k: v for k, v in model.state_dict().items() if not k.startswith(\"fc\")}, strict=False)\n",
    "    ft_model.eval()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        ft_optimizer = torch.optim.SGD(ft_model.parameters(), lr=0.1 * LEARNING_RATE, momentum=args['MOMENTUM'], weight_decay=args['WEIGHT_DECAY'])\n",
    "        for images, labels in val_loader:\n",
    "            ft_optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = ft_model(images)\n",
    "                loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(ft_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            wandb.log({\"val_loss\": loss.item()}, step=log_it)\n",
    "            log_it += 1\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    wandb.log({\"lr\": lr_scheduler.get_last_lr()[0]}, step=log_it)\n",
    "\n",
    "torch.save(model.state_dict(), \"simclr_pretrained.pth\")\n",
    "\n",
    "ft_model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "ft_model.load_state_dict({k: v for k, v in model.state_dict().items() if not k.startswith(\"fc\")}, strict=False)\n",
    "ft_model.eval()\n",
    "finetune_model(ft_model, train_loader, test_loader, args, device, \"simclr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263e547",
   "metadata": {},
   "source": [
    "## 3. t-SNE [1.5 pts]\n",
    "\n",
    "Using the t-SNE method, visualize the embeddings of images from the training and test samples that are obtained from supervised, SimCLR and BYOL models. The output of the average pooling of the model is taken as embeddings. The points corresponding to each of the 10 classes should be plotted with the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ce7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48069b",
   "metadata": {},
   "source": [
    "## 4. Linear probing [1 pts]\n",
    "\n",
    "Train a linear probe for self-supervised models, compare the quality with supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1e40a",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning [1.5 pts]\n",
    "\n",
    "Finally, fine-tune the self-supervised models to STL-10 classification. If you did everything correctly, the quality should be several percent higher than that of the baseline. Similar to task 3, draw how the embeddings changed after the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08512c76",
   "metadata": {},
   "source": [
    "## 6. OOD robustness [2 pts]\n",
    "\n",
    "Now, we have 5 different models:\n",
    "\n",
    "- Supervised\n",
    "- SimCLR + linear probing\n",
    "- SimCLR + fine-tuning\n",
    "- BYOL + linear probing\n",
    "- BYOL + fine-tuning\n",
    "\n",
    "We will compare the models by robustness on out-of-distribution objects. As an OOD dataset, we will take the CIFAR-10 test sample, which has almost the same classes as STL-10 (9/10 classes).\n",
    "The only mismatch in CIFAR-10 is the \"frog\" class, so drop the images of this class. Compare the trained models by OOD accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc441af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8b243",
   "metadata": {},
   "source": [
    "## Bonus. MoCo [2 pts]\n",
    "\n",
    "As a bonus, let's look at another contrastive self-supervised model, MoCov2, from [He et al, 2019](https://arxiv.org/pdf/1911.05722), [He et al, 2020](https://arxiv.org/pdf/2003.04297.pdf). Conduct all the experiments described above with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e515c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet-backward-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
