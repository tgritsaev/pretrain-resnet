{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce608ad",
   "metadata": {},
   "source": [
    "# LSDL CUB, Homework 3. Constrastive Learning [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122249",
   "metadata": {},
   "source": [
    "This task is dedicated to contrastive self-supervised methods. We will focus on the SimCLR and BYOL algorithms that were discussed in class. We will conduct experiments on the [STL10](https://cs.stanford.edu/~acoates/stl10/) dataset, which is ideal for pretraining without labels, as it contains 100k unlabeled, 5k training labeled and 8k test labeled images.\n",
    "\n",
    "To submit the task, you must conduct the experiments described in this notebook and write a report on them in PDF format. Along with the report, the code that allows you to run the experiments must be submitted. Before implementing anything, read all the experiment statements and think about how to better organize the code, do not forget to checkpoint the necessary trained models. We reserve the right to lower the grade for poorly structured code. Be sure to use the **training optimizations** from the [list](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html) in your pipelines, for example, Automatic Mixed Precision, to speed up the experiments. Also note that the report is a **mandatory part** of the assessment, without it we will not check your assignment. The report must include training curves for all the models you run. Also, make sure that your figures and graphs are readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0716cb",
   "metadata": {},
   "source": [
    "## 0. Supervised baseline [0 pts]\n",
    "\n",
    "**If not completed, the maximum for the entire task is 0 points**\n",
    "\n",
    "We will start our study by training a supervised model from a random initial approximation. Use the labeled train for training, and the labeled test for testing. We will use ResNet-18\\* as the neural network architecture. We recommend searching for hyperparameters and augmentations for training on STL-10 in articles. The author of the task got an accuracy of about 71-72%.\n",
    "\n",
    "\\**For datasets with a smaller image size than ImageNet (such as CIFAR-10/100, STL-10), it is common to use ResNet-18 with modified first layers: usually, 7x7 convolution is replaced with 3x3, and 2x2 MaxPooling is removed. We suggest you train the regular ResNet-18 in the torchvision implementation to save time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fe534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:45<15:34,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Test Accuracy: 33.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:30<14:53,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Test Accuracy: 49.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [02:16<14:06,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Test Accuracy: 50.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [03:02<13:18,  9.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Test Accuracy: 54.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [03:49<12:35, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Test Accuracy: 64.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [04:35<11:43, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Test Accuracy: 64.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [05:21<10:51, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Test Accuracy: 67.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [06:29<15:00, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Test Accuracy: 69.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [07:50<15:42, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Test Accuracy: 70.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [09:12<14:39, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Test Accuracy: 72.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [10:34<13:12, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Test Accuracy: 70.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [11:56<11:43, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Test Accuracy: 72.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [13:17<10:15, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Test Accuracy: 72.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [14:39<08:49, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Test Accuracy: 73.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [16:01<07:22, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Test Accuracy: 74.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [17:23<05:53, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Test Accuracy: 74.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [18:34<03:38, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Test Accuracy: 74.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [19:21<01:49, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Test Accuracy: 74.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [20:08<00:51, 10.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Test Accuracy: 74.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:54<00:00, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Test Accuracy: 74.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import trange\n",
    "import wandb\n",
    "\n",
    "PROJECT_NAME = \"HW 3, LSDL 2024. CUB.\"\n",
    "wandb.init(project=PROJECT_NAME, name=\"Supervised\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_GAMMA = 0.955\n",
    "EPOCHS = 100\n",
    "\n",
    "# Configuration\n",
    "NUM_WORKERS = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# Main code\n",
    "model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "\n",
    "mean_stats = [0.485, 0.456, 0.406]\n",
    "std_stats = [0.229, 0.224, 0.225]\n",
    "# from the previous homework\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats),  # Normalize the tensor,\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats)  # Normalize the tensor\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.STL10(root=\"./\", split='train', download=True, transform=train_transforms)\n",
    "test_set = torchvision.datasets.STL10(root=\"./\", split='test', download=True, transform=test_transforms)    \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_GAMMA)\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "log_it = 0\n",
    "\n",
    "for i in trange(EPOCHS):\n",
    "    model.train()\n",
    "    for images, labels in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        wandb.log({\"train_loss\": loss.item()}, step=log_it)\n",
    "        log_it += 1\n",
    "        \n",
    "    model.eval()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # print(f'Epoch {i+1}/{EPOCHS}, Test Accuracy: {accuracy:.2f}')\n",
    "        wandb.log({\"test_accuracy\": accuracy}, step=log_it)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "torch.save(model.state_dict(), \"ft_model.pth\")\n",
    "ft_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0f325",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mft_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ft_accuracy \u001b[38;5;241m=\u001b[39m accuracy\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/pandas/compat/__init__.py:29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Bind the name/qualname attributes of the function.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/pandas/compat/pyarrow.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     _pa_version \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     11\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(_pa_version)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/pyarrow/__init__.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[1;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[1;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# results_df = pd.DataFrame({\"ft\": [accuracy]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ad13e",
   "metadata": {},
   "source": [
    "## 1. SimCLR [1.5 pts]\n",
    "\n",
    "Implement and train the SimCLR method from [Chen et al, 2020](https://arxiv.org/pdf/2002.05709.pdf). We want you to implement it yourself, so this task does not allow you to borrow code from open sources. Use the unlabeled part of STL-10 as a training set, and use the labeled train to validate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿\n",
    "\n",
    "# From the paper:\n",
    "# Default setting. Unless otherwise specified, for data augmentation we use random crop and resize (with random\n",
    "# flip), color distortions, and Gaussian blur (for details, see\n",
    "# Appendix A). We use ResNet-50 as the base encoder network, and a 2-layer MLP projection head to project the\n",
    "# representation to a 128-dimensional latent space. As the\n",
    "# loss, we use NT-Xent, optimized using LARS with learning\n",
    "# rate of 4.8 (= 0.3 × BatchSize/256) and weight decay of\n",
    "# 10−6\n",
    "# . We train at batch size 4096 for 100 epochs.3 Furthermore, we use linear warmup for the first 10 epochs,\n",
    "# and decay the learning rate with the cosine decay schedule\n",
    "# without restarts (Loshchilov & Hutter, 2016).\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.3 * BATCH_SIZE / 256\n",
    "print(f\"LEARNING_RATE: {LEARNING_RATE}\")\n",
    "NUM_WARMUP_STEPS = 10\n",
    "\n",
    "\n",
    "# Appendix A:\n",
    "# In our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random\n",
    "# flip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations\n",
    "# are provided below.\n",
    "\n",
    "# Random crop and resize to 224x224 We use standard Inception-style random cropping (Szegedy et al., 2015). The\n",
    "# crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of\n",
    "# 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to the original size. This has been implemented in Tensorflow as “slim.preprocessing.inception_preprocessing.distorted_bounding_box_crop”, or in Pytorch\n",
    "# as “torchvision.transforms.RandomResizedCrop”. Additionally, the random crop (with resize) is always followed by a\n",
    "# random horizontal/left-to-right flip with 50% probability. This is helpful but not essential. By removing this from our default\n",
    "# augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs\n",
    "\n",
    "# Color distortion Color distortion is composed by color jittering and color dropping. We find stronger color jittering\n",
    "# usually helps, so we set a strength parameter.\n",
    "\n",
    "# Gaussian blur This augmentation is in our default policy. We find it helpful, as it improves our ResNet-50 trained for\n",
    "# 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample\n",
    "# σ ∈ [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\n",
    "\n",
    "# from the orginal paper\n",
    "def get_color_distortion(s=1.0):\n",
    "    # s is the strength of color distortion.\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort\n",
    "\n",
    "simclr_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    get_color_distortion(),\n",
    "    transforms.GaussianBlur(kernel_size=int(0.1 * 224)),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats),  # Normalize the tensor,\n",
    "])\n",
    "\n",
    "class DoubleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        image1 = self.transform(image)\n",
    "        image2 = self.transform(image)\n",
    "        return torch.stack([image1, image2]), label\n",
    "\n",
    "simclr_train_set = DoubleImageDataset(torchvision.datasets.STL10(root=\"./\", split='train+unlabeled', download=True), transform=simclr_transforms)\n",
    "simclr_train_loader = torch.utils.data.DataLoader(simclr_train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Model for SimCLR\n",
    "model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "output_after_avgpool = None\n",
    "# Define the hook function\n",
    "def hook_fn(module, input, output):\n",
    "    global output_after_avgpool\n",
    "    output_after_avgpool = output\n",
    "# Register the hook to the avgpool layer\n",
    "hook = model.avgpool.register_forward_hook(hook_fn)\n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(512, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 128),\n",
    ")\n",
    "\n",
    "\n",
    "# Create the LambdaLR scheduler with the warmup lambda function\n",
    "def linear_warmup_lambda(current_step):\n",
    "    if current_step < NUM_WARMUP_STEPS:\n",
    "        # Linear increase: current_step / num_warmup_steps scales from 0 to 1\n",
    "        return current_step / NUM_WARMUP_STEPS\n",
    "    else:\n",
    "        # After warmup, maintain the base learning rate\n",
    "        return LR_GAMMA ** (current_step - NUM_WARMUP_STEPS)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=linear_warmup_lambda)\n",
    "\n",
    "for i in trange(EPOCHS):\n",
    "    model.train()\n",
    "    for images, labels in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        wandb.log({\"train_loss\": loss.item()}, step=log_it)\n",
    "        log_it += 1\n",
    "        \n",
    "    model.eval()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # print(f'Epoch {i+1}/{EPOCHS}, Test Accuracy: {accuracy:.2f}')\n",
    "        wandb.log({\"test_accuracy\": accuracy}, step=log_it)\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "torch.save(model.state_dict(), \"ft_model.pth\")\n",
    "ft_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9271e6",
   "metadata": {},
   "source": [
    "## 2. BYOL [2.5 pts]\n",
    "\n",
    "Similar to the previous task, implement and train the BYOL method from [Grill et al, 2020](https://arxiv.org/pdf/2006.07733.pdf). To check that the projections do not collapse (the variable $z$ from the original paper), plot the standard deviation of $z$ throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263e547",
   "metadata": {},
   "source": [
    "## 3. t-SNE [1.5 pts]\n",
    "\n",
    "Using the t-SNE method, visualize the embeddings of images from the training and test samples that are obtained from supervised, SimCLR and BYOL models. The output of the average pooling of the model is taken as embeddings. The points corresponding to each of the 10 classes should be plotted with the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ce7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48069b",
   "metadata": {},
   "source": [
    "## 4. Linear probing [1 pts]\n",
    "\n",
    "Train a linear probe for self-supervised models, compare the quality with supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1e40a",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning [1.5 pts]\n",
    "\n",
    "Finally, fine-tune the self-supervised models to STL-10 classification. If you did everything correctly, the quality should be several percent higher than that of the baseline. Similar to task 3, draw how the embeddings changed after the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08512c76",
   "metadata": {},
   "source": [
    "## 6. OOD robustness [2 pts]\n",
    "\n",
    "Now, we have 5 different models:\n",
    "\n",
    "- Supervised\n",
    "- SimCLR + linear probing\n",
    "- SimCLR + fine-tuning\n",
    "- BYOL + linear probing\n",
    "- BYOL + fine-tuning\n",
    "\n",
    "We will compare the models by robustness on out-of-distribution objects. As an OOD dataset, we will take the CIFAR-10 test sample, which has almost the same classes as STL-10 (9/10 classes).\n",
    "The only mismatch in CIFAR-10 is the \"frog\" class, so drop the images of this class. Compare the trained models by OOD accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc441af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8b243",
   "metadata": {},
   "source": [
    "## Bonus. MoCo [2 pts]\n",
    "\n",
    "As a bonus, let's look at another contrastive self-supervised model, MoCov2, from [He et al, 2019](https://arxiv.org/pdf/1911.05722), [He et al, 2020](https://arxiv.org/pdf/2003.04297.pdf). Conduct all the experiments described above with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e515c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet-backward-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
