{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce608ad",
   "metadata": {},
   "source": [
    "# LSDL CUB, Homework 3. Constrastive Learning [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7122249",
   "metadata": {},
   "source": [
    "This task is dedicated to contrastive self-supervised methods. We will focus on the SimCLR and BYOL algorithms that were discussed in class. We will conduct experiments on the [STL10](https://cs.stanford.edu/~acoates/stl10/) dataset, which is ideal for pretraining without labels, as it contains 100k unlabeled, 5k training labeled and 8k test labeled images.\n",
    "\n",
    "To submit the task, you must conduct the experiments described in this notebook and write a report on them in PDF format. Along with the report, the code that allows you to run the experiments must be submitted. Before implementing anything, read all the experiment statements and think about how to better organize the code, do not forget to checkpoint the necessary trained models. We reserve the right to lower the grade for poorly structured code. Be sure to use the **training optimizations** from the [list](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html) in your pipelines, for example, Automatic Mixed Precision, to speed up the experiments. Also note that the report is a **mandatory part** of the assessment, without it we will not check your assignment. The report must include training curves for all the models you run. Also, make sure that your figures and graphs are readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0716cb",
   "metadata": {},
   "source": [
    "## 0. Supervised baseline [0 pts]\n",
    "\n",
    "**If not completed, the maximum for the entire task is 0 points**\n",
    "\n",
    "We will start our study by training a supervised model from a random initial approximation. Use the labeled train for training, and the labeled test for testing. We will use ResNet-18\\* as the neural network architecture. We recommend searching for hyperparameters and augmentations for training on STL-10 in articles. The author of the task got an accuracy of about 71-72%.\n",
    "\n",
    "\\**For datasets with a smaller image size than ImageNet (such as CIFAR-10/100, STL-10), it is common to use ResNet-18 with modified first layers: usually, 7x7 convolution is replaced with 3x3, and 2x2 MaxPooling is removed. We suggest you train the regular ResNet-18 in the torchvision implementation to save time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fe534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hdilab/miniconda3/envs/gflownet-backward-py/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "LR_GAMMA = 0.955\n",
    "EPOCHS = 100\n",
    "\n",
    "# Configuration\n",
    "NUM_WORKERS = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# Main code\n",
    "model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "\n",
    "mean_stats = [0.485, 0.456, 0.406]\n",
    "std_stats = [0.229, 0.224, 0.225]\n",
    "# from the previous homework\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats),  # Normalize the tensor,\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts image to PyTorch tensor with values in [0, 1]\n",
    "    transforms.Normalize(mean=mean_stats, std=std_stats)  # Normalize the tensor\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.STL10(root=\"./\", split='train', download=True, transform=train_transforms)\n",
    "test_set = torchvision.datasets.STL10(root=\"./\", split='test', download=True, transform=test_transforms)    \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_GAMMA)\n",
    "\n",
    "for i in trange(EPOCHS):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct, total = 0, 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            accuracy = correct / total\n",
    "            print(f'Epoch {i+1}/{EPOCHS}, Test Accuracy: {accuracy:.2f}')\n",
    "        \n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ad13e",
   "metadata": {},
   "source": [
    "## 1. SimCLR [1.5 pts]\n",
    "\n",
    "Implement and train the SimCLR method from [Chen et al, 2020](https://arxiv.org/pdf/2002.05709.pdf). We want you to implement it yourself, so this task does not allow you to borrow code from open sources. Use the unlabeled part of STL-10 as a training set, and use the labeled train to validate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9271e6",
   "metadata": {},
   "source": [
    "## 2. BYOL [2.5 pts]\n",
    "\n",
    "Similar to the previous task, implement and train the BYOL method from [Grill et al, 2020](https://arxiv.org/pdf/2006.07733.pdf). To check that the projections do not collapse (the variable $z$ from the original paper), plot the standard deviation of $z$ throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263e547",
   "metadata": {},
   "source": [
    "## 3. t-SNE [1.5 pts]\n",
    "\n",
    "Using the t-SNE method, visualize the embeddings of images from the training and test samples that are obtained from supervised, SimCLR and BYOL models. The output of the average pooling of the model is taken as embeddings. The points corresponding to each of the 10 classes should be plotted with the same color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ce7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48069b",
   "metadata": {},
   "source": [
    "## 4. Linear probing [1 pts]\n",
    "\n",
    "Train a linear probe for self-supervised models, compare the quality with supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1e40a",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning [1.5 pts]\n",
    "\n",
    "Finally, fine-tune the self-supervised models to STL-10 classification. If you did everything correctly, the quality should be several percent higher than that of the baseline. Similar to task 3, draw how the embeddings changed after the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08512c76",
   "metadata": {},
   "source": [
    "## 6. OOD robustness [2 pts]\n",
    "\n",
    "Now, we have 5 different models:\n",
    "\n",
    "- Supervised\n",
    "- SimCLR + linear probing\n",
    "- SimCLR + fine-tuning\n",
    "- BYOL + linear probing\n",
    "- BYOL + fine-tuning\n",
    "\n",
    "We will compare the models by robustness on out-of-distribution objects. As an OOD dataset, we will take the CIFAR-10 test sample, which has almost the same classes as STL-10 (9/10 classes).\n",
    "The only mismatch in CIFAR-10 is the \"frog\" class, so drop the images of this class. Compare the trained models by OOD accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc441af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8b243",
   "metadata": {},
   "source": [
    "## Bonus. MoCo [2 pts]\n",
    "\n",
    "As a bonus, let's look at another contrastive self-supervised model, MoCov2, from [He et al, 2019](https://arxiv.org/pdf/1911.05722), [He et al, 2020](https://arxiv.org/pdf/2003.04297.pdf). Conduct all the experiments described above with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e515c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gflownet-backward-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
